{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERRIEN Samuel, LANCO Antoine, DE OLIVEIRA Joe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXO1\n",
    "\n",
    "\n",
    "Partons de l'exercice 3 (à faire de préférence avant)\n",
    "\n",
    "1) Expliciter et expliquer l'hypothèse faite lors de la construction du classifieur. En particulier, chaque pixel est considéré comme une     variable aléatoire et donc une image comme la réalisation d'un vecteur aléatoire. Quelle hypothèse est faite sur la dépendance entre les pixels ? Quel est l'impact sur la matrice de covariance ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On fera l’hypothèse que les données (VA) sont indépendantes et identiquement distribuées. Chaque image représente une expérience aléatoire\n",
    "- On fait donc l’hypothèse que les pixels sont fortement indépendants entre eux . Cela implique que la matrice de covariance n’aura de valeur forte qu’en diagonale : les pixels ne seront corrélés qu’avec eux même.\n",
    "- Le classifieur dit naïf fait donc l’hypothèse que tout indépendant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Proposer une mesure permettant d'évaluer cette hypothèse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./index.png)\n",
    "La matrice de covariance peut se résumer à une diagonale : chaque pixel n'est corrélé qu'avec lui-même"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Évaluons cette hypothèse en considérant comme voisinage pour chaque pixel, son voisin de gauche et de droite.\n",
    "\n",
    "- Une valeur de la réalisation du vecteur de V.As n’est plus un pixel simple mais l’ensemble contenant lui-même et celui de gauche et droite\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Si nous faisions l'hypothèse qu'un pixel ne dépend que de ses voisins immédiats de gauche et de droite, écrire les paramètres à estimer et comment les estimer. On pourra se contenter de dépendances simples: si une image est un vecteur, un pixel (une composante du vecteur) dépend du pixel avant et après dans le vecteur; pour les pixels sur les bords du vecteurs peuvent être ignoré si besoin.\n",
    "\n",
    "$ P(Y=y | X=x)\\,\\alpha\\,P(X = x | Y=y) * P(Y=y) $\n",
    "\n",
    "$ P(X = x | Y=y) * P(Y=y) = (\\prod_{i = 1}^{d} \\pi{_{i, y, x_{i-1}, x_{i+1}}}^{x_{i}}(1-\\pi_{i, y, x_{i-1}, x_{i+1}})^{1-x_{i}})P(Y = y) $\n",
    "\n",
    "Avec :\n",
    "$ \\pi_{i,y,a,b} = \\frac{\\left | \\left \\{ elements\\,de\\,la\\,classe\\,y\\,dont\\,x_{i}\\,est\\,vrai\\,et\\,x_{i-1}=a\\,et\\,x_{i+1}=b \\right \\} \\right |}{\\left | \\left \\{ elements\\,de\\,la\\,classe\\,y\\,tels\\,que\\,x_{i-1}=a\\,et\\,x_{i+1}=b \\right \\} \\right |} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) BONUS: Coder et expérimenter ce modèle\n",
    "    \n",
    "Pour répondre à l'avant dernière question, une solution est de considérer que la probabilité d'une image est la probabilité jointe sur l'ensemble des pixels. Sans hypothèse, cette probabilité se décompose avec la règle de la chaine. Puis il est possible d'introduire les hypothèses d'indépendance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% de mauvaises réponses : 7.32\n",
      "% de mauvaises réponses (lissage avec α = 1e-09 ) : 4.71\n"
     ]
    }
   ],
   "source": [
    "import pickle, gzip\n",
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "# Load the dataset\n",
    "f = gzip.open('./mnist.pkl.gz', 'rb')\n",
    "u = pickle._Unpickler(f)\n",
    "u.encoding = 'latin1'\n",
    "p = u.load()\n",
    "train_set, valid_set, test_set = p\n",
    "labels = train_set[1]\n",
    "# K = nombre de classes\n",
    "K = 10  \n",
    "    \n",
    "# Calcule priors\n",
    "priors = {}\n",
    "for k in range(K):\n",
    "    priors[k] = np.log(len(labels[labels == k]) / len(labels))\n",
    "    \n",
    "# binarise les images\n",
    "def mk_bin(images):\n",
    "    res = []\n",
    "    for img in images:\n",
    "        tmp = np.zeros(img.shape)\n",
    "        tmp[img > 0.5] = 1\n",
    "        res.append(tmp)\n",
    "    return np.asarray(res)\n",
    "\n",
    "images = mk_bin(train_set[0])\n",
    "labels = train_set[1]\n",
    "\n",
    "images_test = mk_bin(test_set[0])\n",
    "labels_test = test_set[1]\n",
    "\n",
    "# probabilité d'un pixel selon ses voisins de gauche et droite\n",
    "beta = np.ndarray((K, 784, 2, 2))\n",
    "\n",
    "# calcul de l'estimation selon les pixels de gauche et droite\n",
    "for y, img in enumerate(images):\n",
    "    for i in range(1, len(img) - 1):\n",
    "        x_i_m = int(img[i - 1]) # pixel de gauche\n",
    "        x_i_p = int(img[i + 1]) # pixel de droite\n",
    "        beta[labels[y],i,x_i_m,x_i_p] += img[i] # On ajoute le pixel en fonction de ses voisins\n",
    "        \n",
    "# On transforme en probabilité\n",
    "for k in range(K):\n",
    "    beta[k,:,0,0] /= beta[k,:,0,0].sum()\n",
    "    beta[k,:,1,0] /= beta[k,:,1,0].sum()\n",
    "    beta[k,:,0,1] /= beta[k,:,0,1].sum()\n",
    "    beta[k,:,1,1] /= beta[k,:,1,1].sum()\n",
    "\n",
    "\n",
    "# Prediction avec la formule naive bayes\n",
    "def predict_naive_bayes(img, beta):\n",
    "    naive_bayes = {}\n",
    "    for k in range(K):\n",
    "        # P(Y = y)\n",
    "        p_Y_eq_y = priors[k]\n",
    "        tmp = 0\n",
    "        for i in range(1, len(img) - 1):\n",
    "            # On récupère les pixels gauche et droit\n",
    "            x_i_m = int(img[i - 1])\n",
    "            x_i_p = int(img[i + 1])\n",
    "            # On récupère la probabilité associée\n",
    "            piiy = beta[k,i,x_i_m,x_i_p]\n",
    "            # On ne veut pas de probabilité nulle pour le log\n",
    "            if piiy == 0:\n",
    "                continue\n",
    "            # Calcul de la probabilité\n",
    "            tmp += img[i] * math.log(piiy) + (1 - img[i]) * math.log(1-piiy)\n",
    "        naive_bayes[k] = tmp + p_Y_eq_y\n",
    "    return naive_bayes\n",
    "\n",
    "\n",
    "# Evaluation du modele\n",
    "loss = 0.\n",
    "for i in range(len(test_set[0][:1000])):\n",
    "    pred = predict_naive_bayes(test_set[0][i], beta)\n",
    "    res = max(pred.items(), key=operator.itemgetter(1))[0]\n",
    "    loss += 1 if res != test_set[1][i] else 0\n",
    "loss /= len(test_set[1])\n",
    "print(\"% de mauvaises réponses :\",loss * 100)\n",
    "\n",
    "\n",
    "\n",
    "# lissage\n",
    "alpha = 1e-9\n",
    "beta = np.ndarray((10,784,2,2))\n",
    "for y, img in enumerate(images):\n",
    "    for i in range(1, len(img) - 1):\n",
    "        x_i_m = int(img[i - 1])\n",
    "        x_i_p = int(img[i + 1])\n",
    "        beta[labels[y],i,x_i_m,x_i_p] += img[i] + alpha\n",
    "for k in range(10):\n",
    "    beta[k,:,0,0] /= beta[k,:,0,0].sum() + K * alpha\n",
    "    beta[k,:,1,0] /= beta[k,:,1,0].sum() + K * alpha\n",
    "    beta[k,:,0,1] /= beta[k,:,0,1].sum() + K * alpha\n",
    "    beta[k,:,1,1] /= beta[k,:,1,1].sum() + K * alpha\n",
    "    \n",
    "# Evaluation du modele (lissage)\n",
    "loss = 0.\n",
    "for i in range(len(test_set[0][:1000])):\n",
    "    pred = predict_naive_bayes(test_set[0][i], beta)\n",
    "    res = max(pred.items(), key=operator.itemgetter(1))[0]\n",
    "    loss += 1 if res != test_set[1][i] else 0\n",
    "loss /= len(test_set[1])\n",
    "print(\"% de mauvaises réponses (lissage avec α =\", alpha, \") :\",loss * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXO2\n",
    "\n",
    "Repartons du TP sur Bayésien Naif Gaussien appliqué aux images MNIST.\n",
    "\n",
    "Le classifieur vu en TP considère la réalisation un pixel conditionnellement à une classe comme une variable aléatoire gaussienne.\n",
    "\n",
    "- Prenez les images de la classe 4, prendre 5 pixels avec une forte variance et tracer pour chacun la répartition des valeurs avec un histogramme.\n",
    "\n",
    "\n",
    "- Faites la même chose pour la classe 5.\n",
    "\n",
    "\n",
    "- Commenter les résultats obtenus\n",
    "\n",
    "Supposons maintenant que la valeur d'un pixel conditionnellement à sa classe est en fait une variable aléatoire dont la distribution est donné par le mélange de 2 gaussiennes:\n",
    "\n",
    "        Xi|Y=y∼αi,y,1N(μi,y,1,σi,y,1)+αi,y,2N(μi,y,2,σi,y,2)\n",
    "        p(xi|y)=αi,y,1p1(x|y)+αi,y,2p2(x|y),\n",
    "\n",
    "avec p1(x|y)=N(μi,y,1,σi,y,1). La distribution est en fait l'interpolation linéaire de 2 gaussiennes, et :\n",
    "    αi,y,1+αi,y,2=1\n",
    "La question qui se pose est comment calculer les paramètres de ce nouveau modèle pour une classe y donnée, à savoir :\n",
    "\n",
    "        (αi,y,1,αi,y,2,μi,y,1,μi,y,2,σi,y,1,σi,y,2)\n",
    "\n",
    "et nous allons voir comment au travers des questions suivantes.\n",
    "\n",
    "Prenons par exemple les images de la classe 5 et un pixel du centre de l'image (par exemple un des pixels que vous avez sélectionné dans une des questions précédentes). Dans le cas à une seule gaussienne, chaque réalisation contribue à l'estimation des statistiques nécessaires (μ,σ).\n",
    "Avec 2 gaussiennes, on ne sais pas à gaussienne, une observation va contribuer. Mais on peut calculer cette contribution, si on connait et fixe les paramètres.\n",
    "\n",
    "- Proposer une fonction permettant de calculer la contribution d'une réalisation à chacune des gaussienne. Sachant que la somme des deux contributions est 1 et qu'elles dépendent de αi,y,1,αi,y,1,p1,p2.\n",
    "\n",
    "\n",
    "- Une fois connue cette contribution, il est possible de mettre à jour les paramètres, puisque l'on sait pour chaque réalisation, de combien elle participe à chaque gaussienne. Une fois les contribution de chaque réalisation calculée sur les données d'apprentissage, écrire la formule d'estimation des paramètres pour\n",
    "\n",
    "        (αi,y,1,αi,y,2,μi,y,1,μi,y,2,σi,y,1,σi,y,2).\n",
    "\n",
    "Ainsi, si les paramètres sont connus, il est possible de calculer les contributions de chaque exemple, puis de recalculer les paramètres. Ainsi l'algorithme d'apprentissage est itératif:\n",
    "\n",
    "- 0 : initialisation des contributions (tirage au hasard entre les deux gaussiennes)\n",
    "- 1 : Calcul des paramètres.\n",
    "- 2 : Calcul des contributions et retour à l'étape 1\n",
    "\n",
    "Cet algorithme s'appelle E.M, et vous allez l'implémenter avec 2 fonctions à coder et à tester.\n",
    "\n",
    "- Partir des données MNIST et les images de la classe 5, estimer les paramètres du mélange, visualiser la distribution obtenues pour les pixels sélectionnés et comparer cette fonction à l'histogramme des valeurs.\n",
    "\n",
    "\n",
    "- Recommencer avec plusieurs initialisations aléatoires pour comparer les résultats. Sont-ils stables ?\n",
    "\n",
    "\n",
    "- Faire de même avec la classe 4\n",
    "\n",
    "\n",
    "- BONUS : coder un classifier qui utilise un mélange de 2 gaussiennes pour modéliser Xi|Y , évaluer ses performances et comparer avec le classifieur de base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réponses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Proposer une fonction permettant de calculer la contribution d'une réalisation à chacune des gaussienne. Sachant que la somme des deux contributions est 1 et qu'elles dépendent de αi,y,1,αi,y,1,p1,p2.\n",
    "\n",
    "$ \\LARGE p_{1}(X_{i}\\,|\\,Y) = \\frac{\\alpha_{1,i,y}\\,\\mathcal{N}(\\mu_{1,i,y}, \\sigma_{1,i,y})}{\\alpha_{1,i,y}\\,\\mathcal{N}(\\mu_{1,i,y}, \\sigma_{1,i,y}) + \\alpha_{2,i,y}\\,\\mathcal{N}(\\mu_{2,i,y}, \\sigma_{2,i,y})} $\n",
    "\n",
    "$ \\LARGE p_{2}(X_{i}\\,|\\,Y) = 1 - p_{1}(X_{i}\\,|\\,Y) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Une fois connue cette contribution, il est possible de mettre à jour les paramètres, puisque l'on sait pour chaque réalisation, de combien elle participe à chaque gaussienne. Une fois les contribution de chaque réalisation calculée sur les données d'apprentissage, écrire la formule d'estimation des paramètres pour\n",
    "    (αi,y,1,αi,y,2,μi,y,1,μi,y,2,σi,y,1,σi,y,2).\n",
    "    \n",
    "$ \\LARGE \\mu_{1,i,y} = \\frac{\\sum_{j=1}^{N}p_{1}(X_{i}^{(j)}\\,|\\,Y)X_{i}^{(j)}}{\\sum_{j=1}^{N}p_{1}(X_{i}^{(j)}\\,|\\,Y)} $\n",
    "\n",
    "$ \\LARGE \\sigma_{1,i,y} = \\frac{\\sum_{j=1}^{N}p_{1}(X_{i}^{(j)}\\,|\\,Y)(X_{i}^{(j)}-\\mu_{1,i,y})^{2}}{\\sum_{j=1}^{N}p_{1}(X_{i}^{(j)}\\,|\\,Y)} $\n",
    "\n",
    "où $ X_{i}^{(j)} $ est les i-eme pixel de la j-eme image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:130: RuntimeWarning: overflow encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n"
     ]
    }
   ],
   "source": [
    "import pickle, gzip\n",
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "import random\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "# Load the dataset\n",
    "f = gzip.open('./mnist.pkl.gz', 'rb')\n",
    "u = pickle._Unpickler(f)\n",
    "u.encoding = 'latin1'\n",
    "p = u.load()\n",
    "train_set, valid_set, test_set = p\n",
    "\n",
    "# K = nombre de classes\n",
    "\n",
    "# Récupération des images et des labels\n",
    "images = train_set[0][:100]\n",
    "labels = train_set[1][:100]\n",
    "\n",
    "\n",
    "means = {}\n",
    "variances = {}\n",
    "priors = {}\n",
    "\n",
    "means1 = np.random.rand(10,784)\n",
    "means2 = np.random.rand(10,784)\n",
    "var1   = np.random.rand(10,784)\n",
    "var2   = np.random.rand(10,784)\n",
    "alpha  = np.empty((10,784))\n",
    "alpha[:]  = 0.5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, 10):\n",
    "    means[i]     = images[labels==i].mean(axis=0)\n",
    "    variances[i] = images[labels==i].var(axis=0)\n",
    "    priors[i]    = len(images[labels==i])/len(images)\n",
    "    \n",
    "\n",
    "        \n",
    "def computePosteriors(image,means,variances,priors):\n",
    "    res = np.zeros(10)\n",
    "    for lbl in range(10):\n",
    "        mean = means[lbl]\n",
    "        sigma2 = variances[lbl]\n",
    "        non_null = sigma2!=0\n",
    "        \n",
    "        scale = 0.5*np.log(2*sigma2[non_null]*math.pi)\n",
    "        expterm = -0.5*np.divide(np.square(image[non_null]-mean[non_null]), sigma2[non_null])\n",
    "        \n",
    "        llh = (expterm - scale).sum()\n",
    "        post = llh + np.log(priors[lbl])\n",
    "        res[lbl]=post\n",
    "    return res\n",
    "\n",
    "\n",
    "def computeGauss(means,var,x):\n",
    "    \n",
    "    scale = 1/math.sqrt(2*math.pi*var)\n",
    "    expterm = math.exp(-(((x-means)**2)/(2*var)))    \n",
    "    return scale*expterm\n",
    "\n",
    "\n",
    "\n",
    "def winrate(images_test,labels_test):\n",
    "    res = 0\n",
    "    for i in range(len(images_test)):\n",
    "        l = list(computePosteriors(images_test[i],means,variances,priors))\n",
    "        res += 1 if labels_test[i] != l.index(max(l)) else 0\n",
    "        \n",
    "    return 1-(res/len(images_test))\n",
    "\n",
    "images_test = test_set[0]\n",
    "labels_test = test_set[1]\n",
    "\n",
    "def getMax(array,nb):\n",
    "    res = np.zeros(nb)\n",
    "    for i in range(nb):\n",
    "        max = np.argmax(array)\n",
    "        res[i] = max\n",
    "        array[max] = 0\n",
    "    return res\n",
    "    \n",
    "    \n",
    "def getline(array,line):\n",
    "    res = np.zeros(int(array.size/array[0].size))\n",
    "    for i in range(res.size):\n",
    "        res[i] = array[i][int(line)]\n",
    "            \n",
    "    return res\n",
    "    \n",
    "    \n",
    "\n",
    "#print(\"Win Rate : \" + str(winrate(images_test,labels_test)*100) + \" %\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "for j in range(3,4):\n",
    "    var = variances[j]\n",
    "    M = getMax(var,5)\n",
    "    print(\"------------\"+str(j)+\"-------------\")\n",
    "    for i in M:    \n",
    "        plt.hist(getline(images[labels == j], i), log=True)\n",
    "        plt.show()\n",
    "'''    \n",
    "for it in range(5):\n",
    "    #---------contribution-------------\n",
    "    p1 = np.empty((10,len(images),784))\n",
    "    for i in range(10):\n",
    "        for j in range(len(images)):\n",
    "            for k in range(784):\n",
    "                p1[i][j][k] = (alpha[i][k]*computeGauss(means1[i][k],var1[i][k],images[j][k])) / \\\n",
    "                    ((alpha[i][k]*computeGauss(means1[i][k],var1[i][k],images[j][k]))+\\\n",
    "                    ((1-alpha[i][k])*computeGauss(means2[i][k],var2[i][k],images[j][k])))\n",
    "\n",
    "    p2 = 1-p1\n",
    "\n",
    "    #--------maj--------\n",
    "\n",
    "    means1 =np.true_divide((p1*images).sum(axis=1),p1.sum(axis=1))\n",
    "    means2 = np.true_divide((p2*images).sum(axis=1),p2.sum(axis=1))\n",
    "\n",
    "\n",
    "    for i in range(10):\n",
    "        for j in range(len(images)):\n",
    "            for k in range(784):\n",
    "                var1[i][k] += p1[i][j][k]*((images[j][k] - means1[i][k])**2)\n",
    "                var2[i][k] += p2[i][j][k]*((images[j][k] - means2[i][k])**2)\n",
    "\n",
    "    var1 = np.true_divide(var1,p1.sum(axis=1))\n",
    "    var2 = np.true_divide(var2,p2.sum(axis=1))\n",
    "\n",
    "    alpha  = np.true_divide(p1.sum(axis=1),len(images))\n",
    "\n",
    "print(\"fin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXO3 Bayésien naïf discret\n",
    "\n",
    "Reprendre la même tâche mais en binarizant les images. Pour cela, le modèle de chaque pixel est une binomiale. \n",
    "\n",
    "## TODO\n",
    "- Quels sont les paramètres à estimer pour chaque classe ? \n",
    "- Comment les stockés ? \n",
    "- Implémenter l'estimation, puis l'inférence. (On peut dans un premier temps réutiliser l'implémentation existante de Sklearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% de mauvaises réponses : 17.97\n",
      "% de mauvaises réponses (lissage avec α = 1e-9 ) : 15.959999999999999\n"
     ]
    }
   ],
   "source": [
    "import pickle, gzip\n",
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "# Load the dataset\n",
    "f = gzip.open('./mnist.pkl.gz', 'rb')\n",
    "u = pickle._Unpickler(f)\n",
    "u.encoding = 'latin1'\n",
    "p = u.load()\n",
    "train_set, valid_set, test_set = p\n",
    "labels = train_set[1]\n",
    "# K = nombre de classes\n",
    "K = 10  \n",
    "    \n",
    "# Calcule priors\n",
    "priors = {}\n",
    "for k in range(K):\n",
    "    priors[k] = np.log(len(labels[labels == k]) / len(labels))\n",
    "    \n",
    "# binarise les images\n",
    "def mk_bin(images):\n",
    "    res = []\n",
    "    for img in images:\n",
    "        tmp = np.zeros(img.shape)\n",
    "        tmp[img > 0.5] = 1\n",
    "        res.append(tmp)\n",
    "    return np.asarray(res)\n",
    "\n",
    "images = mk_bin(train_set[0])\n",
    "labels = train_set[1]\n",
    "\n",
    "images_test = mk_bin(test_set[0])\n",
    "labels_test = test_set[1]\n",
    "\n",
    "beta = np.ndarray((K, 784))\n",
    "\n",
    "\n",
    "# calcul de l'estimation\n",
    "for y, img in enumerate(images):\n",
    "    beta[labels[y]] += img\n",
    "for k in range(K):\n",
    "    beta[k] /= beta[k].sum()\n",
    "    \n",
    "# Prediction avec la formule naive bayes\n",
    "def predict_naive_bayes(img, beta):\n",
    "    naive_bayes = {}\n",
    "    for k in range(K):\n",
    "        p_Y_eq_y = priors[k]\n",
    "        non_null = beta[k] != 0\n",
    "        xi = img[non_null]\n",
    "        piiy = beta[k][non_null]\n",
    "        tmp = (xi * np.log(piiy) + (1-xi) * np.log(1-piiy) ).sum()\n",
    "        naive_bayes[k] = tmp + p_Y_eq_y\n",
    "    return naive_bayes\n",
    "\n",
    "# Evaluation du modele\n",
    "loss = 0.\n",
    "for i in range(len(test_set[0])):\n",
    "    pred = predict_naive_bayes(test_set[0][i], beta)\n",
    "    res = max(pred.items(), key=operator.itemgetter(1))[0]\n",
    "    loss += 1 if res != test_set[1][i] else 0\n",
    "loss /= len(test_set[1])\n",
    "print(\"% de mauvaises réponses :\",loss * 100)\n",
    "\n",
    "# lissage\n",
    "alpha = 1e-9\n",
    "beta = np.ndarray((10,784))\n",
    "for y, img in enumerate(images):\n",
    "    beta[labels[y]] += img + alpha\n",
    "for k in range(10):\n",
    "    beta[k] /= beta[k].sum() + K * alpha\n",
    "    \n",
    "# Evaluation du modele (lissage)\n",
    "loss = 0.\n",
    "for i in range(len(test_set[0])):\n",
    "    pred = predict_naive_bayes(test_set[0][i], beta)\n",
    "    res = max(pred.items(), key=operator.itemgetter(1))[0]\n",
    "    loss += 1 if res != test_set[1][i] else 0\n",
    "loss /= len(test_set[1])\n",
    "print(\"% de mauvaises réponses (lissage avec α = 1e-9 ) :\",loss * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
